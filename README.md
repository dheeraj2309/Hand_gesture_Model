# Real-Time Hand Gesture and Speech Recognition Control System

This project implements a real-time system that recognizes hand gestures using a custom-trained neural network and performs speech recognition. Recognized speech can optionally be processed by Google's Generative AI (Gemini) for a simple Yes/No response. The system uses OpenCV for video capture, MediaPipe for hand tracking, TensorFlow/Keras for the gesture model, and the SpeechRecognition library for audio input.

## Features

*   **Real-time Hand Gesture Recognition:** Detects and classifies hand gestures from a live webcam feed.
    *   Supported gestures: Backward, Down, Forward, Left, Right, Up.
*   **Speech Recognition:** Captures audio from a microphone and transcribes it to text.
*   **Generative AI Integration :** Sends recognized speech to Google Gemini (1.5 Flash model) for a Yes/No type answer.
*   **Modular Control:** Gesture recognition and speech recognition can be toggled on/off independently during runtime.
*   **Data Preparation and Training Scripts:** Includes scripts to extract hand landmark data from images and train the gesture recognition model.
## Project Structure
<pre><code>
project-root/
├── gesture_recog.h5                       # Trained Keras model (generated by train_ANN.py)
├── landmarkdata_extraction.py             # Script to extract hand landmarks from images
├── live_prediction.py                     # Main script for real-time prediction
├── pics/                                  # Directory for training images
│   ├── Backward/                          # Folder for 'Backward' gesture images
│   │   ├── img1.jpg
│   │   └── ...                            # (other images for this gesture)
│   ├── Down/
│   ├── Forward/
│   ├── Left/
│   ├── Right/
│   └── Up/
├── requirements.txt                       # Python dependencies
├── train_ANN.py                           # Script to train the gesture recognition model
└── training.npz                           # Saved landmark data and labels (generated by landmarkdata_extraction.py)
</code></pre>
## Prerequisites

*   Python 3.8+
*   A webcam
*   A microphone
*   A Google AI API Key for Gemini (if you want to use the speech-to-AI feature)

## Setup and Installation

1.  **Clone the repository:**
    ```bash
    git clone <https://github.com/dheeraj2309/Hand_gesture_Model>
    cd <Hand_gesture_Model>
    ```

2.  **Create a virtual environment (recommended):**
    ```bash
    python -m venv venv
    # On Windows
    venv\Scripts\activate
    # On macOS/Linux
    source venv/bin/activate
    ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    *Note: The `requirements.txt` lists `tensorflow` and `tensorflow_intel`. If you don't have an Intel CPU optimized for it, you might only need `tensorflow`.*

4.  **Set up Google AI API Key:**
    If you want to use the Gemini integration for speech, create a `.env` file in the root directory of the project:
    ```
    API_KEY=YOUR_GOOGLE_AI_API_KEY
    ```
    Replace `YOUR_GOOGLE_AI_API_KEY` with your actual API key from Google AI Studio. The `live_prediction.py` script uses `python-dotenv` to load this key.

## Workflow

The project involves three main steps:

1.  **Data Preparation:** Collect images for each gesture and extract hand landmarks.
2.  **Model Training:** Train a neural network on the extracted landmarks.
3.  **Live Prediction:** Run the main script to use the trained model for real-time gesture and speech recognition.

### 1. Data Preparation (`landmarkdata_extraction.py`)

This script extracts hand landmark data from images and saves it as `training.npz`.

1.  **Create the `pics/` directory** in the project root.
2.  Inside `pics/`, create subdirectories for each gesture you want to train. The script expects the following class names based on `live_prediction.py`'s `class_mapping`:
    *   `Backward`
    *   `Down`
    *   `Forward`
    *   `Left`
    *   `Right`
    *   `Up`
3.  Populate each gesture subdirectory with images of that hand gesture. Ensure only the hand is clearly visible.
4.  Run the script:
    ```bash
    python landmarkdata_extraction.py
    ```
    This will process images in the `pics/` directory and create `training.npz` containing the landmark data and corresponding labels.

### 2. Model Training (`train_ANN.py`)

This script loads `training.npz`, trains an Artificial Neural Network (ANN), and saves the trained model as `gesture_recog.h5`.

1.  Ensure `training.npz` (generated in the previous step) is present in the project root.
2.  Run the script:
    ```bash
    python train_ANN.py
    ```
    This will train the model and save `gesture_recog.h5`.

### 3. Live Prediction (`live_prediction.py`)

This is the main application script for real-time gesture and speech recognition.

1.  Ensure `gesture_recog.h5` (generated in the previous step) is present in the project root.
2.  Ensure your `.env` file with the `API_KEY` is set up for Gemini.
3.  Run the script:
    ```bash
    python live_prediction.py
    ```

    A window will open showing your webcam feed.

#### Controls:

*   **`t`**: Toggle hand gesture recognition ON/OFF.
*   **`s`**: Toggle speech recognition ON/OFF.
    *   When turned ON, it will listen for speech. Recognized text and Gemini's (Yes/No) response will be displayed.
*   **`q`**: Quit the application.

## How it Works

*   **Hand Gesture Recognition:**
    *   MediaPipe's `Hands` solution detects and tracks hand landmarks in real-time from the webcam feed.
    *   The (x, y, z) coordinates of these landmarks are flattened and fed into the pre-trained Keras model (`gesture_recog.h5`).
    *   The model predicts the gesture class.
*   **Speech Recognition:**
    *   The `SpeechRecognition` library listens for audio via the microphone.
    *   It uses Google's Web Speech API (via `recognize_google`) to convert speech to text.
    *   The recognized text is then sent to the Google Gemini API (if enabled and configured) with a prompt "Just answer in Yes or No Q:{recognized_text}? A:" to get a simple Yes/No type answer.

## Customization

*   **Gestures:** To add or change gestures, you'll need to:
    1.  Update the `pics/` directory structure with new gesture image folders.
    2.  Collect new image data.
    3.  Re-run `landmarkdata_extraction.py`.
    4.  Modify the output layer of the neural network in `train_ANN.py` (e.g., `Dense(num_classes, activation='softmax')`) to match the new number of gestures.
    5.  Update the `class_mapping` dictionary in `live_prediction.py`.
    6.  Re-run `train_ANN.py` to train the new model.
*   **Neural Network Architecture:** The ANN in `train_ANN.py` is simple. You can experiment with different numbers of layers, neurons, activation functions, or even different model types (e.g., LSTMs for temporal gesture recognition) for potentially better accuracy.
*   **Speech AI Prompt:** You can modify the prompt sent to Gemini in `live_prediction.py` if you want different types of responses.

## Dependencies

Key libraries used:

*   `opencv-python` / `opencv-contrib-python`: For camera access and image processing.
*   `mediapipe`: For hand landmark detection.
*   `tensorflow`: For building and running the neural network.
*   `scikit-learn`: For data preprocessing (LabelEncoder).
*   `SpeechRecognition`: For speech-to-text.
*   `PyAudio`: As an audio input backend for SpeechRecognition (often needed).
*   `google-generativeai`: For interacting with Google Gemini.
*   `python-dotenv`: For loading environment variables (API key).

See `requirements.txt` for the full list.

## Troubleshooting

*   **`No module named '_tkinter'` (Linux):** You might need to install Tkinter: `sudo apt-get install python3-tk`.
*   **Microphone issues:** Ensure your microphone is correctly configured in your operating system and not muted. `PyAudio` is a common dependency for `SpeechRecognition` to access the microphone; make sure it's installed correctly.
*   **Camera not found:** Ensure your webcam is connected and drivers are installed. OpenCV might sometimes have trouble with specific camera indices; you can try changing `cv.VideoCapture(0)` to `cv.VideoCapture(1)` (or other indices) in `live_prediction.py`.
*   **Low accuracy:**
    *   Ensure good lighting and a clear background when collecting training images and during live prediction.
    *   Collect more diverse training data for each gesture.
    *   Experiment with the neural network architecture or training parameters.
